{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wfdb\n",
      "  Downloading wfdb-4.3.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: aiohttp>=3.10.11 in /opt/conda/lib/python3.12/site-packages (from wfdb) (3.13.3)\n",
      "Requirement already satisfied: fsspec>=2023.10.0 in /opt/conda/lib/python3.12/site-packages (from wfdb) (2026.2.0)\n",
      "Requirement already satisfied: matplotlib>=3.2.2 in /opt/conda/lib/python3.12/site-packages (from wfdb) (3.9.2)\n",
      "Requirement already satisfied: numpy>=1.26.4 in /opt/conda/lib/python3.12/site-packages (from wfdb) (1.26.4)\n",
      "Requirement already satisfied: pandas>=2.2.3 in /opt/conda/lib/python3.12/site-packages (from wfdb) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.8.1 in /opt/conda/lib/python3.12/site-packages (from wfdb) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.13.0 in /opt/conda/lib/python3.12/site-packages (from wfdb) (1.15.3)\n",
      "Collecting soundfile>=0.10.0 (from wfdb)\n",
      "  Downloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (16 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp>=3.10.11->wfdb) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp>=3.10.11->wfdb) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp>=3.10.11->wfdb) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp>=3.10.11->wfdb) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp>=3.10.11->wfdb) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp>=3.10.11->wfdb) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp>=3.10.11->wfdb) (1.22.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib>=3.2.2->wfdb) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.12/site-packages (from matplotlib>=3.2.2->wfdb) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib>=3.2.2->wfdb) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib>=3.2.2->wfdb) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib>=3.2.2->wfdb) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.12/site-packages (from matplotlib>=3.2.2->wfdb) (12.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib>=3.2.2->wfdb) (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.12/site-packages (from matplotlib>=3.2.2->wfdb) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas>=2.2.3->wfdb) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas>=2.2.3->wfdb) (2025.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.8.1->wfdb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests>=2.8.1->wfdb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.8.1->wfdb) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests>=2.8.1->wfdb) (2026.1.4)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.12/site-packages (from soundfile>=0.10.0->wfdb) (1.17.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /opt/conda/lib/python3.12/site-packages (from aiosignal>=1.4.0->aiohttp>=3.10.11->wfdb) (4.12.2)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.12/site-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->wfdb) (1.17.0)\n",
      "Downloading wfdb-4.3.1-py3-none-any.whl (163 kB)\n",
      "Downloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m138.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: soundfile, wfdb\n",
      "Successfully installed soundfile-0.13.1 wfdb-4.3.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install wfdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xmltodict\n",
      "  Downloading xmltodict-1.0.4-py3-none-any.whl.metadata (14 kB)\n",
      "Downloading xmltodict-1.0.4-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: xmltodict\n",
      "Successfully installed xmltodict-1.0.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import os\n",
    "from shutil import copyfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "from util import save_checkpoint, save_reg_checkpoint, my_eval_with_dynamic_thresh\n",
    "from finetune_model import ft_12lead_ECGFounder, ft_1lead_ECGFounder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dataset import UKB_Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune ECG for AFIB classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/notebooks/ECGFounder-ukb/finetune_model.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(pth, map_location=device)\n",
      "/opt/conda/lib/python3.12/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 40 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/opt/conda/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "num_lead = 12 # 12-lead ECG or 1-lead ECG \n",
    "\n",
    "gpu_id = 0\n",
    "batch_size = 512\n",
    "lr = 1e-4\n",
    "weight_decay = 1e-5\n",
    "early_stop_lr = 1e-5\n",
    "Epochs = 5\n",
    "df_label_path = '/opt/notebooks/afib_labels_ecgfounder.tsv'\n",
    "ecg_path = '/mnt/project/Bulk/Electrocardiogram/Resting/'\n",
    "tasks = ['has_afib']\n",
    "saved_dir = '/opt/notebooks/results/eval_afib/'\n",
    "\n",
    "device = torch.device('cuda:{}'.format(gpu_id) if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "n_classes = len(tasks)\n",
    "\n",
    "ECGdataset = UKB_Dataset\n",
    "pth = './checkpoint/12_lead_ECGFounder.pth'\n",
    "model = ft_12lead_ECGFounder(device, pth, n_classes,linear_prob=False)\n",
    "\n",
    "df_label = pd.read_csv(df_label_path, sep=\"\\t\")\n",
    "# Splitting the dataset into train, validation, and test sets\n",
    "\n",
    "train_df, test_df = train_test_split(df_label, test_size=0.2, shuffle=False)\n",
    "val_df, test_df = train_test_split(test_df, test_size=0.5, shuffle=False)\n",
    "\n",
    "train_dataset = ECGdataset(data_dir=ecg_path,labels_df=train_df)\n",
    "val_dataset = ECGdataset(data_dir=ecg_path,labels_df=val_df)\n",
    "test_dataset = ECGdataset(data_dir=ecg_path,labels_df=test_df)\n",
    "\n",
    "# Example DataLoader usage\n",
    "trainloader = DataLoader(train_dataset, batch_size=256,num_workers=40, shuffle=True)\n",
    "valloader = DataLoader(test_dataset, batch_size=256,num_workers=40, shuffle=False)\n",
    "testloader = DataLoader(test_dataset, batch_size=256,num_workers=40, shuffle=False)\n",
    "\n",
    "# linear classificaion  ->  linear_prob=True\n",
    "# full fine-tuning  ->  linear_prob=False\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.1, mode='max', verbose=True)\n",
    "\n",
    "### train model\n",
    "best_val_auroc = 0.\n",
    "step = 0\n",
    "current_lr = lr\n",
    "all_res = []\n",
    "pos_neg_counts = {}\n",
    "total_steps_per_epoch = len(trainloader)\n",
    "eval_steps = total_steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/342 [02:23<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 309, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/opt/notebooks/ECGFounder-ukb/dataset.py\", line 79, in __getitem__\n    data = data.squeeze(0)\n          ^^^^^^^^^^^^^^^\nValueError: cannot select an axis to squeeze out which has size not equal to one\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(Epochs):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m### train\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTraining\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_y\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_x\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1344\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1343\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1370\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1370\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/_utils.py:706\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 706\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 309, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/opt/notebooks/ECGFounder-ukb/dataset.py\", line 79, in __getitem__\n    data = data.squeeze(0)\n          ^^^^^^^^^^^^^^^\nValueError: cannot select an axis to squeeze out which has size not equal to one\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(Epochs):\n",
    "    ### train\n",
    "    for batch in tqdm(trainloader,desc='Training'):\n",
    "        input_x, input_y = tuple(t.to(device) for t in batch)\n",
    "        outputs = model(input_x)\n",
    "        loss = criterion(outputs, input_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "\n",
    "        if step % eval_steps == 0:\n",
    "\n",
    "            # val\n",
    "            model.eval()\n",
    "            prog_iter_val = tqdm(testloader, desc=\"Validation\", leave=False)\n",
    "            all_gt = []\n",
    "            all_pred_prob = []\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, batch in enumerate(prog_iter_val):\n",
    "                    input_x, input_y = tuple(t.to(device) for t in batch)\n",
    "                    logits = model(input_x)\n",
    "                    pred = torch.sigmoid(logits)\n",
    "                    all_pred_prob.append(pred.cpu().data.numpy())\n",
    "                    all_gt.append(input_y.cpu().data.numpy())\n",
    "            all_pred_prob = np.concatenate(all_pred_prob)\n",
    "            all_gt = np.concatenate(all_gt)\n",
    "            all_gt = np.array(all_gt)\n",
    "            res_val, res_val_auroc, res_test_sens, res_test_spec, res_test_f1, res_test_auprc, thre = my_eval_with_dynamic_thresh(all_gt, all_pred_prob)\n",
    "            val_auroc = res_val\n",
    "            print('Epoch {} step {}, val: {:.4f}'.format(epoch, step, res_val))\n",
    "\n",
    "            # test\n",
    "            model.eval()\n",
    "            prog_iter_test = tqdm(testloader, desc=\"Testing\", leave=False)\n",
    "            all_gt = []\n",
    "            all_pred_prob = []\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, batch in enumerate(prog_iter_test):\n",
    "                    input_x, input_y = tuple(t.to(device) for t in batch)\n",
    "                    logits = model(input_x)\n",
    "                    pred = torch.sigmoid(logits)\n",
    "                    all_pred_prob.append(pred.cpu().data.numpy())\n",
    "                    all_gt.append(input_y.cpu().data.numpy())\n",
    "            all_pred_prob = np.concatenate(all_pred_prob)\n",
    "            all_gt = np.concatenate(all_gt)\n",
    "            all_gt = np.array(all_gt)\n",
    "            res_test, res_test_auroc, res_test_sens, res_test_spec, res_test_f1, res_test_auprc, thre = my_eval_with_dynamic_thresh(all_gt, all_pred_prob)\n",
    "            \n",
    "            print('Epoch {} step {}, val: {:.4f}, test: {:.4f} '.format(epoch, step, res_val, res_test))\n",
    "\n",
    "            ### save model and res\n",
    "            is_best = bool(val_auroc > best_val_auroc)\n",
    "            if is_best:\n",
    "                best_val_auroc = val_auroc\n",
    "                print('==> Saving a new val best!')\n",
    "                save_checkpoint({\n",
    "                    'epoch': epoch,\n",
    "                    'step': step,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'scheduler': scheduler.state_dict(),\n",
    "                    'val_auroc': val_auroc,\n",
    "                }, saved_dir)\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            #all_res.append(list(res_test_auroc))\n",
    "\n",
    "            for i, task in enumerate(tasks):\n",
    "              pos_count = test_df[task].sum()\n",
    "              neg_count = len(test_df) - pos_count\n",
    "              all_res.append([task, res_test_auroc[i], res_test_sens[i], res_test_spec[i], res_test_f1[i], res_test_auprc[i], thre[i], pos_count, neg_count])\n",
    "\n",
    "            columns = ['Field_ID', 'AUROC', 'sensitivity', 'specificity', 'f1', 'auprc', 'thre', 'pos_num','neg_num']\n",
    "            \n",
    "            \n",
    "            df = pd.DataFrame(all_res, columns=columns)\n",
    "\n",
    "            df.to_csv(os.path.join(saved_dir, f'res.csv'), index=False, float_format='%.5f')\n",
    "            \n",
    "            scheduler.step(val_auroc)\n",
    "            ### early stop\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            if current_lr < early_stop_lr:\n",
    "                print(\"Early stop\")\n",
    "                exit()\n",
    "                \n",
    "            model.train() # set back to train\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
